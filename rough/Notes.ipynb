{
 "metadata": {
  "name": "",
  "signature": "sha256:c827e16a3aed0f38ef75ab6c29e13b16467ce22b2fabaaa8cd325ce4942e2ef3"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Representing $\\vec x\\in\\mathbb R^m$ with neurons"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "According to classical artificial neural network theory, for an input vector $\\vec x=(x_1,...,x_m)\\in\\mathbb R^m$, a neuron $i$ has an output \n",
      "\n",
      "$$a_i=G(\\Sigma _{j=1}^m w_{ij}\\cdot x_j+b_i)$$\n",
      "\n",
      "where $w_1,...,w_m\\in\\mathbb R^m$ are weights on each input value and $b_i\\in\\mathbb R$ is a constant bias term. $G(J)$ is (almost always) a real-valued function and is usually (though not necessarily) non-decreasing; it is sometimes called the transfer function. Popular versions of $G$ familiar from connectionism are threshold functions, sigmoid functions, rectified linear functions etc. What function you choose depends on what you want to do, including how biologically accurate you want your model to be.\n",
      "\n",
      "Anyway, we can make our way towards the NEF way of thinking about neural representation from here. First notice that\n",
      "\n",
      "$$a_i=G(\\vec x\\cdot \\vec w_i+b_i)$$\n",
      "\n",
      "where $\\vec w_i=(w_1,...,w_m)$. Now we can write \n",
      "\n",
      "$$\\vec w_i = \\alpha _i\\vec e_i$$ \n",
      "\n",
      "where $\\alpha _i= |w_i|$ (and consequently $|e_i|=\\left|\\frac{\\vec w_i}{|w_i|}\\right|=1$). So now we have \n",
      "\n",
      "$$a_i=G(\\alpha _i\\vec x\\cdot \\vec e_i+b_i)$$. \n",
      "\n",
      "(Compare this to the Representation lecture for SYDE556.) The difference between this and the above is mostly notational, but it lets us interpret what is going on with the neuron very nicely. First, $\\vec x\\cdot\\vec e_i=|\\vec x||\\vec e_i|\\cos\\theta=|\\vec x|\\cos\\theta$, where $\\theta$ is the angle between $\\vec x$ and $\\vec e_i$. Consequently $|\\vec x\\cdot\\vec e_i|$ is maximal when $x$ and $e_i$ are colinear. We can think of $e_i$ as determining the preferred axis of neuron $i$. Then $|\\vec e_i\\cdot\\vec x|$ is a measure of the size of the component of $\\vec x$ on the axis of $\\vec e_i$ relative to $\\vec e_i$. The sign of $\\vec e_i\\cdot\\vec x$ tells us if $\\vec x$ is generally pointing in the direction of $e_i$ or not. $\\alpha_i$ is a gain term and determines how sensitive the neuron is to a change in $\\vec x$ relative to $e_i$. The sign of $\\alpha _i$ determines whether $e_i$ is the preferred stimulus of neuron $i$. If $\\alpha$ is positive, the input to $i$ is maximized when $\\vec x$ and $\\vec e_i$ are aligned and pointing in the same direction; if $\\alpha _i$ is negative, input is maximized when the two vectors are aligned but point in opposite directions. $b_i$ determines the baseline activation of the neuron $i$ (activation when $\\vec x = 0$).\n",
      "\n",
      "So far we've been discussing the output $a_i$ of a neuron $i$. How well does $i$ represent $\\vec x$? Or rather, how easy is it to recover $\\vec x$ from $a_i$? Well, it depends. If $m$ (the number of dimensions) is anything greater than $1$, we have lost quite bit of information. At best we know how big the $\\vec e_i$ component of $\\vec x$ is. And if $n$ is $1$, the answer depends on $G$.\n",
      "\n",
      "The NEF insight is that population coding helps you overcome this problem. If you were trying to recover $\\vec x$ from $a_i$, you might write \n",
      "\n",
      "$$\\hat x = g(a_i)$$\n",
      "\n",
      "where $\\hat x$ is an estimate of $\\vec x$. The paragraph above should suggest to you that this is probably not going to work out very well, so try instead\n",
      "\n",
      "$$\\hat x = g(a_1,...,a_n)$$ \n",
      "\n",
      "where $a_1,...,a_n$ are each outputs of individual neurons $1$ through $n$. We have a choice about what $g$ to use, but it turns out (surprisingly!) that a linear transformation works just fine (See SYDE556 Representation lecture or _Neural Engineering_ Chapter 2 by Eliasmith and Anderson). Writing $\\vec a=(a_1,...,a_n)$, we get\n",
      "\n",
      "$$g(\\vec a) = D\\vec a$$\n",
      "\n",
      "where $D:m\\times n$ is a matrix of column vectors $\\vec d_1,...,\\vec d_n$. This gets us \n",
      "\n",
      "$$\\hat x = \\Sigma _{i=1}^n a_i\\vec d_i.$$\n",
      "\n",
      "$D$ is chosen to minimize the mean square error of $\\hat x$. For large enough $n$ (number of neurons), we have the decoding error decreasing as $\\frac 1 {n^2}$. Additionally, the NEF models include random noise in the neural signal (to model actual neural noise). This adds an additional error term, which decreases as $\\frac 1 n$. That means that after a certain point, random noise dominates the total estimate error. (See SYDE556 Representation lecture.)\n",
      "\n",
      "We can put all this together into NEF (and Nengo) terminology. Consider $n$ neurons indexed $1,...,n$. Let $\\vec x\\in\\mathbb R^m$ be the input vector to be represented, $a_i$ denote the output of the $i^{\\mathrm{th}}$ neuron and $\\hat x$ be the estimate of $\\vec x$ represented by the neurons $1,..,n$. We have\n",
      "\n",
      "\\begin{align}\n",
      "a_i &= f(\\alpha _i\\vec x\\cdot\\vec e_i + b_i) &&\\mathrm{(encoding\\ equation)}\\\\\n",
      "\\hat x &= \\Sigma_{i=1}^n a_i\\vec d_i &&\\mathrm{(decoding\\ equation).}\n",
      "\\end{align}\n",
      "\n",
      "The vectors $\\vec e_1,...,\\vec e_n$ are called encoders, and $\\vec d_1,...,\\vec d_n$ are decoders. Just as above, $\\alpha _1,...,\\alpha _n$ are gain values, $b_1,...,b_n$ are unit biases and $f$ is the transfer function."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Implementing an image retrieval system"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our overall goal is to implement an image buffer into which we can load images after some basic processing (scaling and translation). (Compare with Kosslyn-syle visual buffer.) We are not going to simulate image memory, but we still need some model of how images will be retrieved, and consequently of how they will be stored. We additionally need to decide how the image will be represented in the image buffer. We will also simply assume that images are mono-chromatic at this stage of the model. Eventually, we want to attach this to SOILIE and add color."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "The image buffer"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If we stick with the connection to the mental imagery approach, our image buffer should look like the visual buffer. So the space represented by it should be roughly circular in shape and the receptive fields of the neurons implementing the buffer should be similar to those in V1. Here is one way to implement something to that effect in NEF terms.\n",
      "\n",
      "Let $B=\\{\\mathrm n_1,...,\\mathrm n_{n_B}\\}$ be the ensemble (set of neurons) implementing the image buffer. Clearly, there are $n_B$ neurons in $B$. For $\\mathrm n_i\\in B$ the output is determined by\n",
      "\n",
      "$$a_i = f(\\alpha _i\\vec x\\cdot\\vec e_i + b_i),$$\n",
      "\n",
      "where $\\vec x$ is the input (a vector representation of the raw image). We know that the receptive field of neurons in V1 look like Gabor filters, so the $e_i$ should reflect that. Moreover, all that is needed to constrain the shape of the image buffer is that the $e_1,...,e_{n_B}$ cover a roughly circular area (i.e. they cover most of the area within a circle).\n",
      "\n",
      "A note about image models: there are different ways to model images, namely a discrete/digital model (pixel arrays) versus a continuous/analog model (functions). For what follows, we work with digital images, though it should not be too difficult to adjust the model to work with the assumption that the image is analog."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Encoders for the image buffer"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's prepare some encoders.\n",
      "\n",
      "Work with the following assumptions:\n",
      "\n",
      "- Images are a set $\\mathcal{Imgs}$ of $\\mathrm{img\\_size}\\times\\mathrm{img\\_size}$ pixel arrays. (Image model)\n",
      "- A pixel is a triple $(x,y,b)$, where $x,y\\in\\{1,...,\\mathrm{img\\_size}\\}$ are pixel coordinates, $b$ is the brightness of the pixel.\n",
      "- The image buffer encodes an image with $n_B$ gabor filters randomly chosen and distributed within approximately half canvas width from the center of the canvas.\n",
      "- Gabors near the periphery are less frequent than in the center."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Create encoder neurons\n",
      "#Some code (a lot of code) is taken from SYDE556, Spatial Representation lecture (lecture 9)\n",
      "\n",
      "#All images will be on an img_size*img_size canvas.\n",
      "img_size = 28\n",
      "\n",
      "#n_B is as in text above.\n",
      "n_B = 2000\n",
      "\n",
      "import numpy\n",
      "import random\n",
      "\n",
      "#Returns a single gabor filter\n",
      "def gabor(size, lambd, theta, psi, sigma, gamma, x_offset, y_offset):\n",
      "    x = numpy.linspace(-1, 1, size)\n",
      "    y = numpy.linspace(-1, 1, size)\n",
      "    X, Y = numpy.meshgrid(x, y)\n",
      "    X = X - x_offset\n",
      "    Y = Y + y_offset\n",
      "\n",
      "    cosTheta = numpy.cos(theta)\n",
      "    sinTheta = numpy.sin(theta)\n",
      "    xTheta = X * cosTheta  + Y * sinTheta\n",
      "    yTheta = -X * sinTheta + Y * cosTheta\n",
      "    e = numpy.exp( -(xTheta**2 + yTheta**2 * gamma**2) / (2 * sigma**2) )\n",
      "    cos = numpy.cos(2 * numpy.pi * xTheta / lambd + psi)\n",
      "    return e * cos\n",
      "\n",
      "#Returns a gabor filter with placement constraints as in text above\n",
      "def make_random_gabor(size):\n",
      "    sigma = random.uniform(0.1, 0.2)\n",
      "    #Choice of r makes gabors stay within half width of center. \n",
      "    #Also, squaring ensures gabors are more frequent near center.\n",
      "    r = (random.uniform(0,1)-sigma)**2\n",
      "    th = random.uniform(0, 2*numpy.pi)\n",
      "    return gabor(size, \n",
      "                  lambd=random.uniform(0.3, 0.8),\n",
      "                  theta=random.uniform(0, 2*numpy.pi),\n",
      "                  psi=random.uniform(0, 2*numpy.pi),\n",
      "                  sigma=sigma,\n",
      "                  gamma=random.uniform(0.7, 1),\n",
      "                  x_offset=r*numpy.cos(th),\n",
      "                  y_offset=r*numpy.sin(th))\n",
      "                \n",
      "encoders_B = [make_random_gabor(img_size) for i in range(n_B)]\n",
      "\n",
      "#normalize encoders\n",
      "encoders_B = [(1/numpy.linalg.norm(i).flatten())*i for i in encoders_B]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Display the resulting filters."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pylab\n",
      "pylab.figure(figsize=(10,8))\n",
      "for i in range(n_B):\n",
      "    w = i%12\n",
      "    h = i/12\n",
      "    pylab.imshow(encoders_B[i], extent=(w, w+0.95, h, h+0.95), interpolation='none',\n",
      "                 vmin=-1, vmax=1, cmap='gray')\n",
      "    pylab.xticks([])\n",
      "    pylab.yticks([])\n",
      "pylab.xlim((0, 12))\n",
      "pylab.ylim((0, n_B/12))\n",
      "    \n",
      "pylab.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Image uses $n_B=200.$\n",
      "\n",
      "<img src=\"files/Figures/gabors_for_visualbuffer.svg\">\n",
      "\n",
      "Check that image buffer is roughly circular."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "e = numpy.copy(encoders)\n",
      "e[e<0] = -e[e<0]\n",
      "pylab.imshow(numpy.sum(e, 0), extent=(-1, 1, -1, 1), interpolation='none', vmin=-1, vmax=1, cmap='gray')\n",
      "pylab.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'encoders' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-3-96ec30091188>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0me\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpylab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'none'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpylab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mNameError\u001b[0m: name 'encoders' is not defined"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The field gets a lot better (in terms of circularity), as you increase $n_B$. Recall that here, $n_B=200$.\n",
      "\n",
      "<img src=\"files/Figures/visualbuffer_field.svg\">"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Storing images for retrieval"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We are going to store many images. It is probably best if we can compress them somehow. It might even be worthwhile to compress the encoders above. It would make the model lighter, but we could argue that it was done for convenience, not for theoretical reasons. Though, we could probably run things fast enough without the compression anyway."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Image encoding using SVD"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's use SVD. SVD gets us an orthogonal basis with which to express each image in the image set, that means that we should be able to recover each image relatively well. For $\\mathrm{Imgs} = [\\mathrm{img}_1,...,\\mathrm{img}_k]$, where $\\mathrm{img}_i$ are column vectors representing each image (we simply flatten the images to get the vectors), SVD gives us\n",
      "\n",
      "\\begin{align}\\mathrm{Imgs} = U \\Sigma V^\\ast\\end{align}\n",
      "\n",
      "Where $U = [u_1,...,u_k]$ are orthogonal basis vectors and $\\Sigma V^\\ast = [\\Sigma v_1,...,\\Sigma v_k]$ are vector representations of $[\\mathrm{img}_1,...,\\mathrm{img}_k]$ under the basis $U^\\ast$. The great thing this lets us do is truncate $U$ (and hence $V$) to get some lossy compression. \n",
      "\n",
      "There is a normalization issue. We would like to use $U$ as a set of encoders for the images, encoders must be normalized, but SVD does not guarantee that $|u_i|=1$. To fix this, let $M$ be the invertible matrix where $$(m_{i,j}) = \\left\\{\\begin{array}{cc}|u_i| & i=j\\\\ 0 & i\\neq j\\end{array}\\right.$$ We then conveniently have $UM^{-1}=\\left[\\frac{u_1}{|u_1|},...,\\frac{u_k}{|u_k|}\\right]$, $M\\Sigma V^\\ast = [M\\Sigma v_1,...,M\\Sigma v_k]$ and $(UM^{-1})(M\\Sigma V^\\ast)=U\\Sigma V^\\ast=\\mathrm{Imgs}$. At this point we can implement a simple system that does more or less what was wanted, but has some issues with regard to plausibility (see below). But first, let's create some more encoders.   "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Code for creating image encoders with SVD"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cPickle \n",
      "import gzip\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy\n",
      "import scipy.sparse\n",
      "from sparsesvd import sparsesvd\n",
      "try:\n",
      "    from PIL import Image\n",
      "except ImportError:\n",
      "    import Image\n",
      "\n",
      "img_size = 28\n",
      "n_B = 7840\n",
      "n_bases = 80\n",
      "\n",
      "# Use MNIST dataset, many images.\n",
      "# Load the dataset\n",
      "f = gzip.open('Pictures/mnist.pkl.gz', 'rb')\n",
      "train_set, valid_set, test_set = cPickle.load(f)\n",
      "f.close()\n",
      "\n",
      "# Get Imgs\n",
      "Imgs = numpy.array([img/numpy.linalg.norm(img) for img in train_set[0]]).T\n",
      "Imgs_csc = scipy.sparse.csc_matrix(Imgs)\n",
      "\n",
      "ut, S, vt = sparsesvd(Imgs_csc, n_bases)\n",
      "M = numpy.diag([numpy.linalg.norm(ut[i,:]) for i in range(ut.shape[0])])\n",
      "#W is M inverse\n",
      "UW = numpy.dot(ut.T, numpy.linalg.inv(M))\n",
      "MSvt = numpy.dot(M, numpy.dot(numpy.diag(S), vt))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Plot an example to see if it worked."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "recovered = numpy.dot(UW, MSvt[:,4])\n",
      "\n",
      "imgMatReshaped = numpy.reshape(recovered, (img_size, img_size), 'F')        \n",
      "plt.imshow(imgMatReshaped.T, cmap='gray')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Not too bad. More encoder vectors will give better results. (Here, we used $\\mathrm{n\\_ bases}=50$.)\n",
      "\n",
      "<img src=\"files/Figures/normalizedSVD9-50.svg\">"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Putting it together I: A simple system"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's build a network $\\mathrm{Net_1}$ using what we have from the previous two sections.\n",
      "\n",
      "Let $\\mathrm{Imgs}$ be the set of images we want to render in $\\mathrm{Net_1}$, and $B$ be a set of gabor encoders as above. Furthermore, let $U,\\ M,\\ \\Sigma ,\\ V^\\ast$ be such that $\\mathrm{Imgs}=(UM^{-1})(M\\Sigma V^\\ast)$ where $U,\\ \\Sigma,\\ V^\\ast$ are as in the section above. \n",
      "\n",
      "$\\mathrm{Net_1}$ has\n",
      "- 1 input called $\\mathrm{ipt}$\n",
      "    * In particular, inputs include $\\mathrm{columns}(M\\Sigma V^\\ast)$, representations of images in $\\mathrm{Imgs}$ under SVD.\n",
      "- 2 ensembles $\\mathrm{buffer},\\ \\mathrm{retriever}$ where\n",
      "    * $\\mathrm{ibuffer.encoders}=B$ (consequently $\\mathrm{ibuffer.neurons}=|B|$)\n",
      "    * $\\mathrm{renderer.encoders}=UM^{-1}$ (consequently $\\mathrm{renderer.neurons}=|\\mathrm{columns}(UM^{-1})|$)\n",
      "- 3 connections $\\mathrm{in},\\ \\mathrm{render},\\ \\mathrm{refresh}$\n",
      "    * $\\mathrm{in}:\\mathrm{ipt}\\mapsto\\mathrm{renderer}$.\n",
      "    * $\\mathrm{render}:\\mathrm{renderer}\\mapsto\\mathrm{ibuffer}$: computes $\\mathrm{img}_i$ from $\\vec a_\\mathrm{renderer}$ and passes it to $\\mathrm{ibuffer}$. \n",
      "    * $\\mathrm{refresh}:\\mathrm{ibuffer}\\mapsto\\mathrm{ibuffer}$: maintains the activity of $\\mathrm{ibuffer}$ stable.\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Nengo script"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#figure out how to run sims outside of gui, run them, collect results.\n",
      "import nengo\n",
      "\n",
      "def render(v):\n",
      "    return numpy.dot(UW, v)\n",
      "\n",
      "Net_1 = nengo.Network(label=\"Net_1\")\n",
      "with Net_1:\n",
      "    ipt = nengo.Node(list(MSvt[:,4]))\n",
      "    \n",
      "    renderer = nengo.Ensemble(img_size**2, \n",
      "                              dimensions=n_bases, \n",
      "                              encoders=UW, \n",
      "                              label=\"renderer\")#, neuron_type=nengo.Direct())\n",
      "    ibuffer = nengo.Ensemble(n_B, \n",
      "                             dimensions=img_size**2, \n",
      "                             #encoders=numpy.array([encoder.flatten() for encoder in encoders_B]), \n",
      "                             label=\"ibuffer\",)#, neuron_type=nengo.Direct())\n",
      "    out = nengo.Node(size_in=img_size**2)\n",
      "    \n",
      "    nengo.Connection(ipt, renderer)\n",
      "    nengo.Connection(renderer, ibuffer, function=render, synapse=0.01)\n",
      "    nengo.Connection(ibuffer, ibuffer, function=lambda x: 0.01*x, synapse=0.01)\n",
      "    \n",
      "    probeIbuffer_input = nengo.Probe(ibuffer, attr=\"input\")\n",
      "    probeIbuffer_output = nengo.Probe(ibuffer, attr=\"decoded_output\")\n",
      "    \n",
      "sim = nengo.Simulator(Net_1)\n",
      "sim.run(0.5)\n",
      "ipt.output = [0]*50\n",
      "sim.run(0.5)\n",
      "ibuffer_inputData = sim.data[probeIbuffer_input]\n",
      "ibuffer_outputData = sim.data[probeIbuffer_output]    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ImportError",
       "evalue": "No module named nengo",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-4-008f77affddd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#figure out how to run sims outside of gui, run them, collect results.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnengo\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mUW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mImportError\u001b[0m: No module named nengo"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "latestReshaped = numpy.reshape(ibuffer_inputData[-1], (img_size, img_size), 'F')\n",
      "plt.imshow(latestReshaped.T, cmap='gray')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "latestReshaped = numpy.reshape(ibuffer_outputData[-1], (img_size, img_size), 'F')\n",
      "plt.imshow(latestReshaped.T, cmap='gray')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}